{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce541945",
   "metadata": {},
   "source": [
    "# Day 3 Programming Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfafaefb",
   "metadata": {},
   "source": [
    "The following exercises will not count towards your course mark, but they provide you with an opportunity to receive feedback on your programming skills in advance of you completing your summative assignments.\n",
    "\n",
    "The goal of the following exercises is to make you apply the concepts and general methods seen in Day 3\n",
    "of the Bootcamp and develop Python scripts to perform NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e4cb7",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06bb49",
   "metadata": {},
   "source": [
    "Create a for loop to count the characters of the following text\n",
    "\n",
    "text = \"NLTK is really cool!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be54eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the text is: 20\n"
     ]
    }
   ],
   "source": [
    "text = \"NLTK is really cool!\"\n",
    "\n",
    "count = 0\n",
    "for char in text:\n",
    "    count += 1\n",
    "\n",
    "print(\"The length of the text is:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e68ea",
   "metadata": {},
   "source": [
    "Create a function to count the number of words that start with a given letter. Use the function to print the number of words that start with the letter \"c\" using the text variable. Feel free to look online and use the following\n",
    "\n",
    "* Regular expressions link: https://www.w3schools.com/python/python_regex.asp\n",
    "* Use the re.search() method\n",
    "* Use the split method to create the tokens\n",
    "* You can implement the function without using a regex library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d24adb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words starting with 'c' is: 2\n",
      "The number of words starting with 'c' is: 2\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Two households, both alike in dignity, In fair Verona, \n",
    "          where we lay our scene, From ancient grudge break to new mutiny, \n",
    "          Where civil blood makes civil hands unclean.\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "texts = text.split()\n",
    "\n",
    "count = 0\n",
    "for word in texts:\n",
    "    c = re.search(\"c\", word)\n",
    "    if c != None:\n",
    "        if c.span()[0] == 0:\n",
    "            count += 1\n",
    "print(\"The number of words starting with 'c' is:\", count)\n",
    "\n",
    "count = 0\n",
    "for word in texts:\n",
    "    if word.startswith(\"c\"):\n",
    "        count += 1\n",
    "\n",
    "print(\"The number of words starting with 'c' is:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99002a1b",
   "metadata": {},
   "source": [
    "Use the NLKT *word_tokenize* method to count the words of the following text\n",
    "\n",
    "text = \"NLTK is really cool!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f38eb4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "text = \"NLTK is really cool!\"\n",
    "\n",
    "len(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9dca6d",
   "metadata": {},
   "source": [
    "Use the NLKT gutenberg.words method to count the unique words of the following text of the gutenberg Corpus\n",
    "\n",
    "book = shakespeare-macbeth.tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "322b6d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\339755\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4017"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "macbeth = gutenberg.words('shakespeare-macbeth.txt')\n",
    "\n",
    "unique_words = []\n",
    "for word in macbeth:\n",
    "    if word not in unique_words:\n",
    "        unique_words.append(word)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf42eb0",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88ea9052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'London': 2, 'is': 1, 'a': 1, 'most': 2}\n"
     ]
    }
   ],
   "source": [
    "# Run and analyse the following commands to create a dictionary containing:\n",
    "# words as keys and a counter showing how many times a word appeared in a text frequency\n",
    "\n",
    "text = \"\"\"London is a London most most\"\"\"\n",
    "\n",
    "# Create an empty dictionary\n",
    "word_freq = dict()\n",
    "\n",
    "# Transform the corpus to a string and then\n",
    "# split the corpus variable to words\n",
    "\n",
    "corpus_word = str(text).split()\n",
    "for index in range(len(corpus_word)):\n",
    "    if corpus_word[index] not in word_freq:\n",
    "        word_freq[corpus_word[index]] = 1\n",
    "    else:\n",
    "        word_freq[corpus_word[index]] += 1\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de1b90",
   "metadata": {},
   "source": [
    "Use the \"word_tokenize\" method to trasnform a text to a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcaf3d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\339755\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Analytic', 'Tools', 'for', 'Data', 'Science']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = 'Analytic Tools for Data Science'\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d8b73",
   "metadata": {},
   "source": [
    "Download the Guteneberg coprus.\n",
    "\n",
    "Print the list of books provided with the gutenber corpus.\n",
    "\n",
    "* The Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1770686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2cb88",
   "metadata": {},
   "source": [
    "What is the total number of words written in the shakespeare-caesar.txt file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bde079e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25833"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caesar = nltk.corpus.gutenberg.words('shakespeare-caesar.txt')\n",
    "\n",
    "len(caesar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e85494",
   "metadata": {},
   "source": [
    "Transform the macbeth text in an NLTK Text object. Use the concordance method to print the **concordances** of the word \"caesar\" using the caesar text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8b50001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 190 matches:\n",
      "           [ The Tragedie of Julius Caesar by William Shakespeare 1599 ] Actus\n",
      "ndeede sir , we make Holyday to see Caesar , and to reioyce in his Triumph Mur\n",
      "uile fearefulnesse . Exeunt . Enter Caesar , Antony for the Course , Calphurni\n",
      "Caes . Calphurnia Cask . Peace ho , Caesar speakes Caes . Calphurnia Calp . He\n",
      "curse Ant . I shall remember , When Caesar sayes , Do this ; it is perform ' d\n",
      ", and leaue no Ceremony out Sooth . Caesar Caes . Ha ? Who calles ? Cask . Bid\n",
      "shriller then all the Musicke Cry , Caesar : Speake , Caesar is turn ' d to he\n",
      "the Musicke Cry , Caesar : Speake , Caesar is turn ' d to heare Sooth . Beware\n",
      " , come from the throng , look vpon Caesar Caes . What sayst thou to me now ? \n",
      "espect in Rome , ( Except immortall Caesar ) speaking of Brutus , And groaning\n",
      "ng ? I do feare , the People choose Caesar For their King Cassi . I , do you f\n",
      "as I my selfe . I was borne free as Caesar , so were you , We both haue fed as\n",
      "d Tyber , chafing with her Shores , Caesar saide to me , Dar ' st thou Cassius\n",
      "could arriue the Point propos ' d , Caesar cride , Helpe me Cassius , or I sin\n",
      " the waues of Tyber Did I the tyred Caesar : And this Man , Is now become a Go\n",
      "ature , and must bend his body , If Caesar carelesly but nod on him . He had a\n",
      "e new Honors , that are heap ' d on Caesar Cassi . Why man , he doth bestride \n",
      "that we are vnderlings . Brutus and Caesar : What should be in that Caesar ? W\n",
      "and Caesar : What should be in that Caesar ? Why should that name be sounded m\n",
      "tus will start a Spirit as soone as Caesar . Now in the names of all the Gods \n",
      "nce , Vpon what meate doth this our Caesar feede , That he is growne so great \n",
      "ch shew of fire from Brutus , Enter Caesar and his Traine . Bru . The Games ar\n",
      "ne . Bru . The Games are done , And Caesar is returning Cassi . As they passe \n",
      "at the matter is Caes Antonio Ant . Caesar Caes Let me haue men about me , tha\n",
      "n are dangerous Ant . Feare him not Caesar , he ' s not dangerous , He is a No\n"
     ]
    }
   ],
   "source": [
    "caesar = nltk.text.Text(caesar)\n",
    "caesar.concordance('caesar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae61a6f",
   "metadata": {},
   "source": [
    "Use \"sents\" method to find the number of sentences in shakespeare-caesar.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "959e640c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2163"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.gutenberg.sents('shakespeare-caesar.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1a124",
   "metadata": {},
   "source": [
    "Run the following command to download and import the books corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b7ec57",
   "metadata": {},
   "source": [
    "What is text1 about? Run the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3028b5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2cf767",
   "metadata": {},
   "source": [
    "Use the concordance method to find the concordance of the words \"Monwtrous\" and \"size\" in the Moby Dick text.\n",
    "\n",
    "Hint: pass the two words as a list to the concordance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2cc3f",
   "metadata": {},
   "source": [
    "Use the collocations method to print the collocations for text5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae19fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32d7c4",
   "metadata": {},
   "source": [
    "Run the following command to create a dispersion plot diagram for the word whale using the Moby Dick text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4975c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text1.dispersion_plot(['whale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5efd9d",
   "metadata": {},
   "source": [
    "Create a dispersion plot diagram using the words JOIN, chat and Player using the chat corpus.\n",
    "\n",
    "Hint: Create a list containg the three words then pass the list to the dispersion_plot method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091acc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51f8b0",
   "metadata": {},
   "source": [
    " Print all the words in the Moby Dick text longer than 16 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b95e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47118ff9",
   "metadata": {},
   "source": [
    "Run the following command to download the stopwords in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9493c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3bb6f",
   "metadata": {},
   "source": [
    "Print the first 10 stopwords in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16349b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ba73b",
   "metadata": {},
   "source": [
    "**Let us use the Pirates of the Carribean dataset, to find how many times the word \"crew\" exists in our data.**\n",
    "\n",
    "To make analysis easier, we can clean our dataset from any stopwords. \n",
    "\n",
    "To run this task there are a series of steps that you will need to perform including:\n",
    "\n",
    "Importing the webtext, that includes the pirates.txt file\n",
    "\n",
    "Create a frequency distribution dictionary, excluding the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb4ece",
   "metadata": {},
   "source": [
    "Create the sentence tokenizer.\n",
    "\n",
    "Use the following text to extract a list of sentences using the sent_tokenize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c12ccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Athens is the capital and largest city of Greece.', \"It dominates the Attica region and is one of the world's oldest cities, with its recorded history spanning over 3,400 years.\", 'Classical Athens was a powerful city-state.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # Install Punkt corpus (required by the sent_tokenize)\n",
    "\n",
    "text = \"\"\"Athens is the capital and largest city of Greece. It dominates the Attica region and is one of the world's oldest cities, with its recorded history spanning over 3,400 years. Classical Athens was a powerful city-state. \"\"\"\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Please provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e834bd",
   "metadata": {},
   "source": [
    "How many sentences are in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04b59b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f93f88",
   "metadata": {},
   "source": [
    "Create the word tokens for each sentence.\n",
    "\n",
    "Use the the word_tokenize method to extract the words (tokens) of the first sentence (Athens is the capital and largest city of Greece.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb446e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Athens', 'is', 'the', 'capital', 'and', 'largest', 'city', 'of', 'Greece', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Please provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6c56a",
   "metadata": {},
   "source": [
    "How many words are in the first sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1625d2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5461574",
   "metadata": {},
   "source": [
    "Let's examine the part of speech for the first sentence. Use the Part of Speech tagger \"pos_tag\" method to print the tag for each word (token) in the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b782e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Athens', 'NNS'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('capital', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('largest', 'JJS'),\n",
       " ('city', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Greece', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Please provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ec8f6",
   "metadata": {},
   "source": [
    "Click on the following link to understand the meanings of some of the tags: https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4616d",
   "metadata": {},
   "source": [
    "Remove the stop words from the first sentence and store the words into a new list called filtered_sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70d0a2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Athens', 'is', 'the', 'capital', 'and', 'largest', 'city', 'of', 'Greece', '.']\n",
      "['Athens', 'capital', 'largest', 'city', 'Greece', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Please provide your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e0eabb",
   "metadata": {},
   "source": [
    "## Python Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5049d",
   "metadata": {},
   "source": [
    "Regular expressions are a powerful language for matching text patterns.\n",
    "This page gives a basic introduction to regular expressions themselves\n",
    "sufficient for our Python exercises and shows how regular expressions\n",
    "work in Python. The Python \"re\" module provides regular expression\n",
    "support.\n",
    "\n",
    "In Python a regular expression search is typically written as:\n",
    "\n",
    "``` python\n",
    "import re\n",
    "match = re.search(pat, str)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac286b6",
   "metadata": {
    "id": "mYOYdzB83glE"
   },
   "source": [
    "The re.search() method takes a regular expression pattern and a string\n",
    "and searches for that pattern within the string. If the search is\n",
    "successful, search() returns a match object or None otherwise.\n",
    "Therefore, the search is usually immediately followed by an if-statement\n",
    "to test if the search succeeded, as shown in the following example which\n",
    "searches for the pattern 'word:' followed by a 3 letter word (details\n",
    "below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9fba7835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found word:cat\n"
     ]
    }
   ],
   "source": [
    "# Please analyse and run the following commands\n",
    "\n",
    "import re\n",
    "str = 'an example word:cat!!'\n",
    "match = re.search(r'word:\\w\\w\\w', str)\n",
    "if match:\n",
    "    print('found', match.group())\n",
    "else:\n",
    "    print('did not find')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061dacc",
   "metadata": {},
   "source": [
    "The code `match = re.search(pat, str)` stores the search result in a\n",
    "variable named `match`. Then the if-statement tests the match -- if true\n",
    "the search succeeded and `match.group()` is the matching text (e.g.\n",
    "'word:cat'). Otherwise if the match is false (None to be more specific),\n",
    "then the search did not succeed, and there is no matching text.\n",
    "\n",
    "The `'r'` at the start of the pattern string designates a python \"raw\" string\n",
    "which passes through backslashes without change which is very handy for regular\n",
    "expressions. I recommend that you always write pattern strings with the `'r'`\n",
    "just as a habit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7aeb89",
   "metadata": {},
   "source": [
    "Repeat the above commands using the pattern \"I love Python\" as the string and \"I \\w\\w\\w\\w\" as the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "844461d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found I love\n"
     ]
    }
   ],
   "source": [
    "# Please provide you solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c27b5",
   "metadata": {},
   "source": [
    "### Basic Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15872d57",
   "metadata": {},
   "source": [
    "The power of regular expressions is that they can specify patterns, not\n",
    "just fixed characters. Here are the most basic patterns which match\n",
    "single chars:\n",
    "\n",
    "-   `a`, `X`, `9`, `<` -- ordinary characters just match themselves exactly.\n",
    "    The meta-characters which do not match themselves because they have\n",
    "    special meanings are: `. ^ $ * + ? { [ ] \\ | ( )`\n",
    "    (details below)\n",
    "-   `.` (a period) -- matches any single character except newline '\\\\n'\n",
    "-   `\\w` -- (lowercase w) matches a \"word\" character: a letter or digit\n",
    "    or underscore `[a-zA-Z0-9\\_]`. Note that although \"word\" is the\n",
    "    mnemonic for this, it only matches a single word char, not a\n",
    "    whole word. `\\W` (upper case W) matches any non-word character.\n",
    "-   `\\b` -- boundary between word and non-word\n",
    "-   `\\s` -- (lowercase s) matches a single whitespace character -- space,\n",
    "    newline, return, tab, form `[ \\n\\r\\t\\f]`. `\\S` (upper case S)\n",
    "    matches any non-whitespace character.\n",
    "-   `\\t`, `\\n`, `\\r` -- tab, newline, return\n",
    "-   `\\d` -- decimal digit `[0-9]` (some older regex utilities do not\n",
    "    support but `\\d`, but they all support `\\w` and `\\s`)\n",
    "-   `^` = start, `$` = end -- match the start or end of the string\n",
    "-   `\\` -- inhibit the \"specialness\" of a character. So, for example,\n",
    "    use `\\.` to match a period or `\\\\` to match a slash. If you are\n",
    "    unsure if a character has special meaning, such as `@`, you can put\n",
    "    a slash in front of it, `\\@`, to make sure it is treated just as\n",
    "    a character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e20ff",
   "metadata": {},
   "source": [
    "The basic rules of regular expression search for a pattern within a\n",
    "string are:\n",
    "\n",
    "-   The search proceeds through the string from start to end, stopping\n",
    "    at the first match found\n",
    "-   All of the pattern must be matched, but not all of the string\n",
    "-   If `match = re.search(pat, str)` is successful, match is not `None`\n",
    "    and in particular `match.group()` is the matching text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29599d39",
   "metadata": {},
   "source": [
    "### Leftmost & Largest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3a483",
   "metadata": {},
   "source": [
    "First the search finds the leftmost match for the pattern, and second it\n",
    "tries to use up as much of the string as possible -- i.e. `+` and `*` go as\n",
    "far as possible (the `+` and `*` are said to be \"greedy\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "94c1b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 9), match='1 2   3'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "## i+ = one or more i's, as many as possible.\n",
    "#match = re.search(r'pi+', 'piiig')\n",
    "\n",
    "## Finds the first/leftmost solution, and within it drives the +\n",
    "## as far as possible (aka 'leftmost and largest').\n",
    "## In this example, note that it does not get to the second set of i's.\n",
    "\n",
    "# Please uncomment the following command, study and analyse the result\n",
    "#match = re.search(r'i+', 'piigiiii')\n",
    "\n",
    "## \\s* = zero or more whitespace chars\n",
    "## Here look for 3 digits, possibly separated by whitespace.\n",
    "\n",
    "# Please uncomment the following command, study and analyse the result\n",
    "#match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx1 2   3xx')\n",
    "\n",
    "# Please uncomment the following command, study and analyse the result\n",
    "#match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx12  3xx')\n",
    "\n",
    "\n",
    "# Please uncomment the following command, study and analyse the result\n",
    "#match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx123xx')\n",
    "\n",
    "## ^ = matches the start of string, so this fails:\n",
    "\n",
    "# Please uncomment the following command, study and analyse the result\n",
    "#match = re.search(r'^b\\w+', 'foobar')\n",
    "\n",
    "## but without the ^ it succeeds:\n",
    "\n",
    "# Please uncomment the following command, study and analyse the result\n",
    "#match = re.search(r'b\\w+', 'foobar')\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9720e7f",
   "metadata": {},
   "source": [
    "# Emails Example\n",
    "\n",
    "Suppose you want to find the email address inside the string \n",
    "' purple alice-b@google.com monkey dishwasher'. We'll use this as a running example\n",
    "to demonstrate more regular expression features. Here's an attempt using\n",
    "the pattern r'\\\\w+@\\\\w+':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f7df4d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b@google\n"
     ]
    }
   ],
   "source": [
    "# Please analyse and run the following commands\n",
    "\n",
    "import re\n",
    "str = 'purple alice-b@google.com monkey dishwasher'\n",
    "match = re.search(r'\\w+@\\w+', str)\n",
    "if match:\n",
    "    print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7053e",
   "metadata": {},
   "source": [
    "## Group Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1789d93",
   "metadata": {},
   "source": [
    "The \"group\" feature of a regular expression allows you to pick out parts of the matching text. Suppose for the emails problem that we want to extract the username and host separately. To do this, add parenthesis () around the username and host in the pattern, like this: r'([\\w.-]+)@([\\w.-]+)'. In this case, the parenthesis do not change what the pattern will match, instead they establish logical \"groups\" inside of the match text. On a successful search, match.group(1) is the match text corresponding to the 1st left parenthesis, and match.group(2) is the text corresponding to the 2nd left parenthesis. The plain match.group() is still the whole match text as usual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "539285e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice-b@google.com\n",
      "alice-b\n",
      "google.com\n"
     ]
    }
   ],
   "source": [
    "# Please analyse and run the following commands\n",
    "\n",
    "str = 'purple alice-b@google.com monkey dishwasher'\n",
    "match = re.search('([\\w.-]+)@([\\w.-]+)', str)\n",
    "if match:\n",
    "    print(match.group())   ## 'alice-b@google.com' (the whole match)\n",
    "    print(match.group(1))  ## 'alice-b' (the username, group 1)\n",
    "    print(match.group(2))  ## 'google.com' (the host, group 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d4c4a",
   "metadata": {},
   "source": [
    "A common workflow with regular expressions is that you write a pattern\n",
    "for the thing you are looking for, adding parenthesis groups to extract\n",
    "the parts you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16161022",
   "metadata": {},
   "source": [
    "## findall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c78927",
   "metadata": {},
   "source": [
    "`findall()` is probably the single most powerful function in the `re`\n",
    "module. Above we used `re.search()` to find the first match for a pattern.\n",
    "`findall()` finds *all* the matches and returns them as a list of\n",
    "strings, with each string representing one match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "126ec556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice@google.com\n",
      "bob@abc.com\n"
     ]
    }
   ],
   "source": [
    "## Suppose we have a text with many email addresses\n",
    "str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n",
    "\n",
    "## Here re.findall() returns a list of all the found email strings\n",
    "emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', str) ## ['alice@google.com', 'bob@abc.com']\n",
    "for email in emails:\n",
    "    # do something with each found email string\n",
    "    print(email)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
