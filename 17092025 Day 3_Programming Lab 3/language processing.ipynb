{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\339755\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\339755\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\339755\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import nltk\n",
    "import urllib3\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the necessary NLTK resources and Spacy model\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set web config settings for urllib, urllib3, requests\n"
     ]
    }
   ],
   "source": [
    "# Internal packages for setting web proxy and BOE configuration, can skip if not using a Bank of England device\n",
    "import boewebconnectpy\n",
    "boewebconnectpy.set_boe_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the text\n",
    "def process_text(text, utf8=True):\n",
    "    '''\n",
    "    Process the input text by cleaning and tokenizing it into sentences.\n",
    "    Parameters:\n",
    "        text (str): The input text to be processed.\n",
    "        utf8 (bool): Flag indicating if the text is in UTF-8 encoding and needs decoding.\n",
    "    '''\n",
    "    if utf8:\n",
    "        text = text.decode('utf-8')\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').replace('\\t', '')\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'{[^>]+}', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\.(\\[a-zA-Z])', r'. \\1', text)\n",
    "    text  = sent_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched data from https://www.bankofengland.co.uk/stress-testing/2025/key-elements-bank-capital\n",
      "Successfully fetched data from https://www.bankofengland.co.uk/stress-testing/2024/stress-testing-uk-banking-system-scenarios-2024-desk-based\n",
      "Successfully fetched data from https://www.bankofengland.co.uk/financial-stability-report/2024/november-2024#section6\n",
      "Successfully fetched data from https://www.bankofengland.co.uk/stress-testing/2024/boes-approach-to-stress-testing-the-uk-banking-system\n"
     ]
    }
   ],
   "source": [
    "# Create a list of relevant URLs to scrape based on the publication\n",
    "url_list = [\"https://www.bankofengland.co.uk/stress-testing/2025/key-elements-bank-capital\",\n",
    "            \"https://www.bankofengland.co.uk/stress-testing/2024/stress-testing-uk-banking-system-scenarios-2024-desk-based\",\n",
    "            \"https://www.bankofengland.co.uk/financial-stability-report/2024/november-2024#section6\",\n",
    "            \"https://www.bankofengland.co.uk/stress-testing/2024/boes-approach-to-stress-testing-the-uk-banking-system\"]\n",
    "\n",
    "# Initialise the dataframe to hold all sentences\n",
    "all_data = pd.DataFrame(columns=['OriginalSentence', 'Source', 'Line'])\n",
    "for url in url_list:\n",
    "    # Fetch the webpage content\n",
    "    r = urllib3.request(\"GET\", url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    # Check if the request was successful\n",
    "    if r.status == 200:\n",
    "        # Print a success message\n",
    "        print(f\"Successfully fetched data from {url}\")\n",
    "        # Process the text\n",
    "        sentences = process_text(r.data)\n",
    "        # Remove the first line as contains the webpage information\n",
    "        sentences.pop(0)\n",
    "        # Create a dataframe for the sentences\n",
    "        sentence_data = pd.DataFrame(sentences, columns=['OriginalSentence'])\n",
    "        # Add source URL and line number\n",
    "        sentence_data['Source'] = url\n",
    "        sentence_data['Line'] = sentence_data.index + 1\n",
    "        # Append to the main dataframe\n",
    "        all_data = pd.concat([all_data, sentence_data], ignore_index=True)\n",
    "    else:\n",
    "        # Print an error message with code for diagnostics\n",
    "        print(f\"Failed to fetch data from {url}, status code: {r.status}\")\n",
    "\n",
    "# Remove any duplicate sentences\n",
    "all_data = all_data.drop_duplicates().reset_index(drop=True)\n",
    "# Remove last line if contains it is 'Skip to main content' as is the bottom page navigation\n",
    "all_data = all_data[~all_data['OriginalSentence'].str.contains('Skip to main content')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the English stopwords from NLTK\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalSentence</th>\n",
       "      <th>Source</th>\n",
       "      <th>Line</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>NoStopwords</th>\n",
       "      <th>Numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stress testing is used by the Bank to determin...</td>\n",
       "      <td>https://www.bankofengland.co.uk/stress-testing...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "      <td>Stress testing used Bank determine UK banking ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By doing so the Bank aims to ensure banks can ...</td>\n",
       "      <td>https://www.bankofengland.co.uk/stress-testing...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>82</td>\n",
       "      <td>Bank aims ensure banks absorb rather amplify s...</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rather, like previous concurrent stress test s...</td>\n",
       "      <td>https://www.bankofengland.co.uk/stress-testing...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>58</td>\n",
       "      <td>Rather , like previous concurrent stress test ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is not a set of events that is expected, or...</td>\n",
       "      <td>https://www.bankofengland.co.uk/stress-testing...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17</td>\n",
       "      <td>set events expected , likely , materialise .</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This tail risk scenario is used for the purpos...</td>\n",
       "      <td>https://www.bankofengland.co.uk/stress-testing...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23</td>\n",
       "      <td>tail risk scenario used purposes enhancing fin...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    OriginalSentence  \\\n",
       "0  Stress testing is used by the Bank to determin...   \n",
       "1  By doing so the Bank aims to ensure banks can ...   \n",
       "2  Rather, like previous concurrent stress test s...   \n",
       "3  It is not a set of events that is expected, or...   \n",
       "4  This tail risk scenario is used for the purpos...   \n",
       "\n",
       "                                              Source  Line  WordCount  \\\n",
       "0  https://www.bankofengland.co.uk/stress-testing...   1.0         29   \n",
       "1  https://www.bankofengland.co.uk/stress-testing...   2.0         82   \n",
       "2  https://www.bankofengland.co.uk/stress-testing...   3.0         58   \n",
       "3  https://www.bankofengland.co.uk/stress-testing...   4.0         17   \n",
       "4  https://www.bankofengland.co.uk/stress-testing...   5.0         23   \n",
       "\n",
       "                                         NoStopwords Numeric  \n",
       "0  Stress testing used Bank determine UK banking ...          \n",
       "1  Bank aims ensure banks absorb rather amplify s...    2025  \n",
       "2  Rather , like previous concurrent stress test ...          \n",
       "3       set events expected , likely , materialise .          \n",
       "4  tail risk scenario used purposes enhancing fin...          "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of the words in the original sentence\n",
    "all_data[\"WordCount\"] = all_data['OriginalSentence'].apply(lambda x: len(word_tokenize(x)))\n",
    "# Remove the stop words from the sentence to reduce noise\n",
    "all_data[\"NoStopwords\"] = all_data['OriginalSentence'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stopwords]))\n",
    "# Use pos tag to remove all numeric values including those in text form\n",
    "all_data['Numeric'] = all_data['OriginalSentence'].apply(lambda x: nltk.pos_tag(word_tokenize(x))).apply(lambda x: ' '.join([word for word, pos in x if pos in ['CD']]))\n",
    "\n",
    "# Remove very long sentences and any known outliers\n",
    "all_data = all_data[all_data['WordCount'] < 100]\n",
    "all_data = all_data[all_data['OriginalSentence'] != 'Measuring the stability of the banking system: capital and liquidity at risk with solvency-liquidity...']\n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a file containing text to be tested against\n",
    "test = open(\"Stress Test 2026.txt\", \"r\")\n",
    "test_data = test.read()\n",
    "test.close()\n",
    "\n",
    "# Process the test text and save to a dataframe\n",
    "test = process_text(test_data, utf8=False)\n",
    "test_df = pd.DataFrame(test, columns=['OriginalSentence'])\n",
    "\n",
    "# Extract out numbers\n",
    "test_df['OriginalNumeric'] = test_df['OriginalSentence'].apply(lambda x: nltk.pos_tag(word_tokenize(x))).apply(lambda x: ' '.join([word for word, pos in x if pos in ['CD']]))\n",
    "# Create a dataframe with only numeric values\n",
    "numeric_df = test_df.copy()\n",
    "numeric_df = numeric_df[numeric_df['OriginalNumeric'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(sentence, all_data, min_threshold=0.75, perfect_threshold = 0.9, stopword = False):\n",
    "    '''\n",
    "    Check the sentence against each sentence in the all_data dataframe for similarity using Spacy's NLP model.\n",
    "    Parameters:\n",
    "        sentence (str): The input sentence to be checked\n",
    "        all_data (DataFrame): The dataframe containing sentences to compare against\n",
    "        min_threshold (float): The minimum similarity threshold to consider a match\n",
    "        perfect_threshold (float): The similarity threshold to consider a perfect match and stop searching for more\n",
    "        stopword (bool): Flag indicating whether to include stopwords in the comparison\n",
    "    Returns:\n",
    "        Similar (str): The most similar sentence found\n",
    "        SimilarSource (str): The source URL of the most similar sentence\n",
    "        SimilarLine (int): The line number of the most similar sentence\n",
    "        SimilarityScore (float): The similarity score of the most similar sentence\n",
    "    '''\n",
    "    # If stopwords are to be removed, filter them out from the input sentence\n",
    "    if stopword == False:\n",
    "        sentence = sentence.join([word for word in word_tokenize(sentence) if word.lower() not in stopwords])\n",
    "    # Convert the input sentence to a Spacy document\n",
    "    doc1 = nlp(sentence)\n",
    "    # Initialise the maximum similarity score\n",
    "    max_similarity = 0\n",
    "    # Iterate through each row in the all_data dataframe to compute similarity\n",
    "    for idx, row in all_data.iterrows():\n",
    "        # Convert the current sentence to a Spacy document, with or without stopwords based on the flag\n",
    "        if stopword == False:\n",
    "            doc2 = nlp(row['NoStopwords'])\n",
    "        else:\n",
    "            doc2 = nlp(row['OriginalSentence'])\n",
    "        # Calculate the similarity score between the two documents\n",
    "        similarity = doc1.similarity(doc2)\n",
    "        # Update the maximum similarity score and store relevant information if a new maximum is found\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            if max_similarity >= perfect_threshold and max_similarity > min_threshold:\n",
    "                # If a perfect match is found, break the loop early and return the result\n",
    "                Similar = row['OriginalSentence']\n",
    "                SimilarSource = row['Source']\n",
    "                SimilarLine = row['Line']\n",
    "                SimilarityScore = max_similarity\n",
    "                break\n",
    "    # After checking all sentences, return the result if above the minimum threshold, otherwise indicate no match found\n",
    "    if max_similarity > min_threshold:\n",
    "        Similar = row['OriginalSentence']\n",
    "        SimilarSource = row['Source']\n",
    "        SimilarLine = row['Line']\n",
    "        SimilarityScore = max_similarity\n",
    "    else:\n",
    "        Similar = \"None found\"\n",
    "        SimilarSource = None\n",
    "        SimilarLine = None\n",
    "        SimilarityScore = None\n",
    "    return Similar, SimilarSource, SimilarLine, SimilarityScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the similarity check function to each sentence in the test dataframe and store the results\n",
    "test_df[[\"Similar\", \"SimilarSource\", \"SimilarLine\", \"SimilarityScore\"]] = test_df['OriginalSentence'].apply(lambda x: check_similarity(x, all_data, min_threshold=0.8, perfect_threshold=0.98)).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numeric_match(row, all_data):\n",
    "    '''\n",
    "    Check the sentence against each sentence in the all_data dataframe for similarity using Spacy's NLP model.\n",
    "    Parameters:\n",
    "        row (Series): The input row containing the sentence and its numeric values\n",
    "        all_data (DataFrame): The dataframe containing sentences to compare against\n",
    "    Returns:\n",
    "        Match (float): The numeric match found\n",
    "        Similar (str): The most similar sentence found\n",
    "        SimilarSource (str): The source URL of the most similar sentence\n",
    "        SimilarLine (int): The line number of the most similar sentence\n",
    "        SimilarityScore (float): The similarity score of the most similar sentence\n",
    "    '''\n",
    "    # Filter the all_data dataframe to only include rows with matching numeric values\n",
    "    all_data['NumericMatch'] = all_data['Numeric'].apply(lambda x: re.search(row['OriginalNumeric'], x) is not None if x else False)\n",
    "    # Create a copy for numeric matches\n",
    "    numeric_matches = all_data.copy()\n",
    "    # Filter only for numeric matches\n",
    "    numeric_matches = numeric_matches[numeric_matches['NumericMatch'] == True]\n",
    "    # If none found return none\n",
    "    if numeric_matches.empty:\n",
    "        return 'None found', None, None, None, None\n",
    "    # Calculate similarity for the numeric matches\n",
    "    doc1 = nlp(row['OriginalSentence'])\n",
    "    numeric_matches['Similarity'] = numeric_matches['OriginalSentence'].apply(lambda x: nlp(x).similarity(doc1))\n",
    "    # Get the best match based on similarity\n",
    "    best_match = numeric_matches.loc[numeric_matches['Similarity'].idxmax()]\n",
    "    # Save the results\n",
    "    Match = best_match['Numeric']\n",
    "    Similar = best_match['OriginalSentence']\n",
    "    SimilarSource = best_match['Source']\n",
    "    SimilarLine = best_match['Line']\n",
    "    SimilarityScore = best_match['Similarity']\n",
    "    return Match, Similar, SimilarSource, SimilarLine, SimilarityScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the numeric matches\n",
    "numeric_df[[\"NumericMatch\", \"SimilarNumeric\", \"NumericSimilarSource\", \"NumericSimilarLine\", \"NumericSimilarityScore\"]] = numeric_df.apply(lambda x: check_numeric_match(x, all_data), axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a CSV file\n",
    "test_df[['AnalystNotes', 'AnalystSignOff', 'SignOffDate', 'IssuesObserved', 'AlternativeSourceUsed']] = \"\"\n",
    "numeric_df[['AnalystNotes', 'AnalystSignOff', 'SignOffDate', 'IssuesObserved', 'AlternativeSourceUsed']] = \"\"\n",
    "# Open the file to write to multiple sheets\n",
    "with pd.ExcelWriter('StressTest2026.xlsx') as excel:\n",
    "    test_df.to_excel(excel, sheet_name='Text', index= False)\n",
    "    numeric_df.to_excel(excel, sheet_name='Numeric', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
